# Improving Cypher Query Generation

This guide covers strategies to improve the quality and syntax correctness of Cypher queries generated by the LLM.

## Model Selection

### Current Setup
- **Model**: `llama3.1:8b` (4.9 GB)
- **Temperature**: 0.2 (good for deterministic output)
- **Provider**: Ollama (local)

### Model Options

#### 1. **Larger Models (Better Instruction Following)**
```bash
# Pull larger models (if you have enough RAM/VRAM)
ollama pull llama3.1:70b    # Best quality, requires ~40GB RAM
ollama pull llama3.1:70b-instruct-q4_0  # Quantized, ~20GB RAM
ollama pull llama3.2:11b    # Good middle ground, ~7GB RAM
```

**Trade-offs:**
- ✅ Better at following complex instructions
- ✅ Better syntax understanding
- ✅ More consistent output
- ❌ Slower generation (2-5x slower)
- ❌ Higher memory requirements

#### 2. **Code-Specialized Models**
```bash
# Code-specific models (if available via Ollama)
ollama pull codellama:13b   # Meta's code model
ollama pull deepseek-coder:6.7b  # Code generation focused
```

**Trade-offs:**
- ✅ Better at code syntax
- ✅ Understands programming patterns
- ❌ May not understand Cypher specifically
- ❌ May need fine-tuning for Cypher

#### 3. **Cypher-Specific Models (HuggingFace)**
These would require integration with HuggingFace transformers:
- `Azzedde/llama3.1-8b-text2cypher` - Fine-tuned for Cypher
- `lakkeo/stable-cypher-instruct-3b` - Specialized for Cypher

**Trade-offs:**
- ✅ Best Cypher syntax accuracy
- ✅ Trained specifically for this task
- ❌ Requires different integration (not Ollama)
- ❌ May need GPU for reasonable speed

### Recommendation
For testing, try:
1. **llama3.2:11b** - Better than 8b, still manageable
2. **llama3.1:70b-instruct-q4_0** - Best quality if you have RAM
3. Keep temperature at 0.2 or lower (0.1 for most deterministic)

## Prompt Engineering Techniques

### 1. **Few-Shot Learning (Examples in Prompt)**

Add concrete examples to your prompts:

```python
# In system_message_generate:
"""
EXAMPLE INPUT:
Text: "Napoleon was born in Corsica and became Emperor of France."

EXAMPLE OUTPUT:
CREATE (n:Person {name: "Napoleon", file_path: "...", repo_path: "..."})
CREATE (c:Place {name: "Corsica", file_path: "...", repo_path: "..."})
CREATE (f:Country {name: "France", file_path: "...", repo_path: "..."})
CREATE (n)-[:BORN_IN {file_path: "...", repo_path: "..."}]->(c)
CREATE (n)-[:RULED {title: "Emperor", file_path: "...", repo_path: "..."}]->(f)
"""
```

### 2. **Explicit Syntax Rules**

Add clear rules to the prompt:
- "Always create nodes before relationships"
- "Never use MATCH after CREATE in the same query"
- "Use CREATE for new nodes, MERGE if node might exist"

### 3. **Structured Output Instructions**

Force specific format:
```
Output format:
1. All CREATE statements first
2. All MERGE statements second  
3. All relationship statements last
4. No explanations, no markdown
```

### 4. **Chain of Thought**

Break down the task:
```
Step 1: Identify all entities
Step 2: Identify all relationships
Step 3: Write CREATE statements for entities
Step 4: Write CREATE statements for relationships
```

### 5. **Negative Examples**

Show what NOT to do:
```
WRONG:
CREATE (a:Node {name: "test"})
MATCH (a)  // ERROR: Can't MATCH after CREATE

CORRECT:
CREATE (a:Node {name: "test", file_path: "...", repo_path: "..."})
CREATE (b:Node {name: "other", file_path: "...", repo_path: "..."})
CREATE (a)-[:RELATES {file_path: "...", repo_path: "..."}]->(b)
```

## Other Techniques

### 1. **Lower Temperature**
```bash
# In .env
LLM_MODEL_TEMPERATURE=0.1  # More deterministic (default: 0.2)
```

Lower temperature = more consistent, less creative output

### 2. **Post-Processing Validation**

Add validation layer to catch and fix common errors:

```python
def fix_common_cypher_errors(cypher: str) -> str:
    """Fix common syntax errors in generated Cypher"""
    # Fix: MATCH after CREATE
    # Fix: Inline node creation in relationships
    # Fix: Missing file_path/repo_path
    # Fix: Invalid property syntax
    return fixed_cypher
```

### 3. **Iterative Refinement**

Generate, validate, ask LLM to fix:
```python
# 1. Generate Cypher
cypher = generate_cypher(text)

# 2. Try to execute
try:
    db.execute(cypher)
except SyntaxError as e:
    # 3. Ask LLM to fix
    fixed = fix_cypher(cypher, str(e))
```

### 4. **Structured Output with JSON**

Some models support structured output:
```python
# Ask for JSON structure, then convert to Cypher
{
    "nodes": [...],
    "relationships": [...]
}
# Then generate Cypher from structure
```

### 5. **Prompt Templates with Constraints**

Use stricter prompt format:
```
<cypher>
[ONLY CYPHER CODE HERE]
</cypher>
```

And in extraction, reject anything outside the tags.

### 6. **Separate Tasks**

Split into two steps:
1. Extract entities/relationships (structured output)
2. Generate Cypher from structure (deterministic)

## Comparison Table

| Technique | Effectiveness | Complexity | Speed Impact |
|-----------|--------------|------------|--------------|
| Larger model | ⭐⭐⭐⭐⭐ | Low | Slower |
| Few-shot examples | ⭐⭐⭐⭐ | Medium | None |
| Explicit rules | ⭐⭐⭐ | Low | None |
| Lower temperature | ⭐⭐⭐ | Very Low | None |
| Post-processing | ⭐⭐⭐⭐ | Medium | Fast |
| Iterative refinement | ⭐⭐⭐⭐⭐ | High | Slower |
| Structured output | ⭐⭐⭐⭐ | High | None |

## Recommended Approach

### Quick Wins (Low Effort, Good Results)
1. ✅ **Lower temperature to 0.1** (already at 0.2, good)
2. ✅ **Add explicit syntax rules** to prompts
3. ✅ **Add few-shot examples** from your mock data
4. ✅ **Improve extraction** (already done ✅)

### Medium Effort (Better Results)
1. **Try larger model**: `llama3.2:11b` or `llama3.1:70b-q4_0`
2. **Add post-processing validation** for common errors
3. **Add negative examples** to prompts

### Higher Effort (Best Results)
1. **Two-stage generation**: Extract structure → Generate Cypher
2. **Iterative refinement**: Generate → Validate → Fix
3. **Fine-tune model** on Cypher examples

## Testing Strategy

1. **Baseline**: Test current setup (llama3.1:8b, current prompts)
2. **Prompt improvements**: Test with improved prompts
3. **Model upgrade**: Test with larger model
4. **Combined**: Test with both improvements

Track:
- Syntax error rate
- Execution success rate
- Query quality (semantic correctness)

## Next Steps

1. **Try improved prompts** (files created: `system_message_generate_improved`, `prompt_generate_improved`)
2. **Test with llama3.2:11b** if available
3. **Add post-processing** for common error patterns
4. **Add few-shot examples** from your actual data

